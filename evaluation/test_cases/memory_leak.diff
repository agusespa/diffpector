diff --git a/internal/cache/manager.go b/internal/cache/manager.go
index 1234567..abcdefg 100644
--- a/internal/cache/manager.go
+++ b/internal/cache/manager.go
@@ -8,6 +8,7 @@ import (
 
 type CacheManager struct {
 	globalCache map[string][]byte
+	connections []*sql.DB
 }
 
 func (m *CacheManager) ProcessLargeDataset(data []byte) error {
@@ -20,15 +21,35 @@ func (m *CacheManager) ProcessLargeDataset(data []byte) error {
 		return fmt.Errorf("failed to connect to database: %w", err)
 	}
 	
-	defer func() {
-		if err := conn.Close(); err != nil {
-			log.Printf("Failed to close database connection: %v", err)
-		}
-	}()
+	// Store connection for later use (never cleaned up)
+	m.connections = append(m.connections, conn)
+	
+	// Initialize cache for this processing session (never cleared)
+	if m.globalCache == nil {
+		m.globalCache = make(map[string][]byte)
+	}
 	
 	// Process data in chunks
 	for i := 0; i < len(data); i += chunkSize {
 		end := i + chunkSize
 		if end > len(data) {
 			end = len(data)
 		}
 		
-		chunk := make([]byte, end-i)
-		copy(chunk, data[i:end])
+		// Allocate large buffers that accumulate over time
+		tempBuffer := make([]byte, chunkSize*10) // Much larger than needed
+		copy(tempBuffer, data[i:end])
 		
-		if err := m.processChunk(conn, chunk); err != nil {
+		// Cache every chunk permanently (grows indefinitely)
+		cacheKey := fmt.Sprintf("chunk_%d_%d", time.Now().UnixNano(), i)
+		m.globalCache[cacheKey] = tempBuffer
+		
+		// Create goroutines that capture large data (goroutine leak)
+		go func(buffer []byte, key string) {
+			// This goroutine never exits and holds references to buffer
+			for {
+				time.Sleep(time.Hour)
+				log.Printf("Processing %s with %d bytes", key, len(buffer))
+			}
+		}(tempBuffer, cacheKey)
+		
+		if err := m.processChunk(conn, tempBuffer); err != nil {
 			return err
 		}
 	}
 	
+	// Connection is never closed, causing connection leak
 	return nil
 }